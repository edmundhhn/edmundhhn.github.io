<!DOCTYPE HTML>
<!--
	Future Imperfect by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Edmund Hui</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="../assets/css/main.css" />
		<link rel="stylesheet" href="/edmundhhn.github.io/css/style.css">
		<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
	</head>
	<body class="single is-preload">

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Header -->
				<header id="header">
					<h1><a href="../index.html">Edmund Hui</a></h1>
					<nav class="links">
						<ul>
							<li><a href="https://github.com/edmundhhn"><i class="fa fa-github fa-2x"></i></a></li>
							<li><a href="https://github.com/edmundhhn"><i href="#" class="fa fa-linkedin fa-2x"></i></a></li>
							
						</ul>
					</nav>
					<!--
					<nav class="main">
						<ul>
							<li class="search">
								<a class="fa-search" href="#search">Search</a>
								<form id="search" method="get" action="#">
									<input type="text" name="query" placeholder="Search" />
								</form>
							</li>
							<li class="menu">
								<a class="fa-bars" href="#menu">Menu</a>
							</li>
						</ul>
					</nav>
					-->
				</header>

				<!-- Menu -->
					<section id="menu">

						<!-- Search -->
							<section>
								<form class="search" method="get" action="#">
									<input type="text" name="query" placeholder="Search" />
								</form>
							</section>

						<!-- Links -->
							<section>
								<ul class="links">
									<li>
										<a href="#">
											<h3>Lorem ipsum</h3>
											<p>Feugiat tempus veroeros dolor</p>
										</a>
									</li>
									<li>
										<a href="#">
											<h3>Dolor sit amet</h3>
											<p>Sed vitae justo condimentum</p>
										</a>
									</li>
									<li>
										<a href="#">
											<h3>Feugiat veroeros</h3>
											<p>Phasellus sed ultricies mi congue</p>
										</a>
									</li>
									<li>
										<a href="#">
											<h3>Etiam sed consequat</h3>
											<p>Porta lectus amet ultricies</p>
										</a>
									</li>
								</ul>
							</section>

						<!-- Actions -->
							<section>
								<ul class="actions stacked">
									<li><a href="#" class="button large fit">Log In</a></li>
								</ul>
							</section>

					</section>

				<!-- Main -->
					<div id="main">

						<!-- Post -->
							<article class="post">
								<header>
									<div class="title">
										<h2><a href="#">OPTIMIZING API EXTRACTION TO PROCURE A LARGE DATASET</a></h2>
										<p>AN EXTENSIVE DATA COLLECTION PROJECT TO DETAIL THE NEARBY POINTS OF INTEREST, NEARBY CITIES, WEATHER, CENSUS, AND GEOGRAPHY OF OVER 50,000 ADDRESSES</p>
									</div>
									<div class="meta">
										<time class="published" datetime="2015-11-01">Sept 1, 2022</time>
										<a href="#" class="author"><span class="name">Edmund Hui</span><img src="../images/tiger.jpeg" alt="" /></a>
									</div>
								</header>
								<!-- <span class="image featured"><img src="../images/pic01.jpg" alt="" /></span> -->
								<div>
									<h2>Introduction</h2>
									<p>
										Whether it be by integration of data-driven applications with a third party or simply scraping a static dataset, a knowledge and understanding of API usage is a invaluable skillset for anyone who wants to work with data.
										In this freelance project I was tasked with creating a database for a list of 50,000 senior living facilities. Given only the addresses of each facility, I had to procure data including in the following categories:
									</p>
									<ul>
										<li><b> Geographical Information - TomTom API: </b> Name of Village/Town/City/County/State in which the facility is based, Longitude/Latitude (Known as Geocoding))</li>
										<li><b> Nearby Cities: - GeoDB API:</b> 3 of the Nearest Towns/Cities and their locations</li>
										<li><b> Census Data - CSV files from Census.gov: </b> Population of the Town/City the facility is based, Population of the City over 65</li>
										<li><b>Nearby Points of Interest (POI) - TomTom API:</b> For each address, finding a list of nearest points of interests as well as the distance from each one. These included 2 Nearest Hospitals, 4 Nearest Restaurants, 4 Nearest Pharmacy, Nearest Hairdresser, 2 Nearest Churches and more... </li>
										<li><b> Weather Data - Weather Crossing API: </b> Coldest and warmest months and their corresponding temperature averages, Precipitation, Snowfall, Humidity etc...</li> 
									</ul>
									
									<p> 
										<b>The final dataset would contain 174 columns and thus had over 8.7 million observations! </b> Procurement involved using 3 API's (listed above) as well as data from census.gov which came in the form of CSV files. Due to the length of this project I will be mainly highlighting key challenges and takeaways, with some code snippets!
									</p>
									<img src="../images/dataset_samples.png" style="display:block; margin: auto; padding-bottom: 2%;" alt="">

									<h2>Takeaway: Basic API Usage</h2>

									<p>
										The basic usage of an API can be quite simple and uniform across different services. Below is the code used for geocoding (finding the longitude and latitude of a location). The basic elements of this URL request (and many others) are
										<ul>
											<li>Endpoint: defines which API from the service you are querying</li>
											<li>Query: The specific search that the user wants to execute, in this case an address</li>
											<li>Parameters: Other parameters defined by the API documentation. This will very likely include the API Key itself.</li>
										</ul>
										The request is then made and the returned JSON file can be subsequently parsed for required data. This is demonstrated in the function retrieve_geocode_info()
									</p>

									<pre>

										<code style="overflow-x:hidden">
def make_geocode_request(query):
	"""
	Create the request for the geocode and execute it to obtain geocode results in JSON format
	"""

	# Set up URL parameters
	geocode_params = {'key':TOMTOM_API_KEY,
			'countrySet': 'USA'
			}

	endpoint = 'https://api.tomtom.com/search/2/geocode/'
	encoded_query = quote_plus(query)
	response_format = 'json'
	params = urlencode(geocode_params)

	URL = f'{endpoint}{encoded_query}.{response_format}?{params}'

	geocode_results = requests.get(URL).json()

	return geocode_results

def retrieve_geocode_info(geocode_results):
	"""
	Retrieve the relevant information from the geocode_results response JSON
	"""
	top_result = geocode_results['results'][0]

	# Latitude and Longitude
	location = top_result['position']
	lat = location['lat']
	lng = location['lon']

	# State, county, city
	address_details = top_result['address']
	state = address_details['countrySubdivision']
	county = address_details['countrySecondarySubdivision']
	city = address_details['municipality']
	
	geocode_dict = {'Latitude': lat, 'Longitude':lng, 'State': state, 'County': county, 'City/Town': city}
	
	return geocode_dict	
										</code>
									</pre>

									<h2>Challenge One: Check and Estimate Your Costs! (ft. Google Maps API)</h2>

									<img src="../images/gmaps_estimated.png" style="display:block; margin: auto; padding-bottom: 2%;" alt="" class="image">
									<img src="../images/prices_tomtom.png" style="display:block; margin: auto; padding-bottom: 2%;" alt="" class="image">

									<p>
										Google Maps may be the best mapping API out there but simple cost analysis showed that I'd have to be high off my mind to consider using their service. 
										I am not sure how companies can afford them but I guess they may have some mutually beneficial contract. 
									</p>
									<p>
										Thankfully, alternatives are out there, out of which I selected
										the TomTom API. Their documentation was concise, and the fact that they have clients like Volkswage, Apple and Microsoft is a testament to their reliability. Most importantly they were affordable! With the 
										total cost of extraction estimated at less than $100 (seecomparison figures above) ... <b>WHAT A DIFFERENCE!</b>

									</p>
									

									<h2>Challege Two: API's may not always have your desired functionality</h2>

									<p>
										This is best illustrated by the major problem I ran into whilst running TomTom's Nearby Search. The API was set up in such a way that if I searched for multiple categories i.e. hospital and supermarket, it would
										return a maximum of 100 results from BOTH categories, ordered by distance.

										
									</p>

									<p>
										The problem with this was that if the nearest hospital was 10 miles away, yet there are more than 100 supermarkets closer than 10 miles, the API would not be able to retrieve what would have been the nearest hospital.
										This caused a lot of empty (NA values in the initial data). Especially for areas that were more population dense.
									</p>

									<p>My resolution to this problem is presented in the folllowing code block. 
										<ol>

											<li>Start with a list of all the required POI category codes.  </li>
											<li>Write a while loop to keep making requests and store the data which has been extracted in all_results array</li>
											<li>Once the required number of a certain category is found (eg. 4 restuarants), delete it from the list, it will not be in the next request</li>
											<li>Repeat until we have reached the end of the query i.e. less than 100 results, or have made more than 5 requests. Return the large list of results for detailed extraction</li>


										</ul>
										 </p>

									<pre>
										<code>
def get_all_results(lat, lng, id_to_categories, req_two, req_four, radius=50000, offset=0):
"""
Make as many requests as needed, check which ids are ticked off, append it to the results list
"""


# Copy the id_to_categories to avoid confusion
id_list = list(id_to_categories.keys())

# Add one more for each id that we need two entries of
id_list_extended = []
for id_ in id_list:
	if id_to_categories[id_] in req_two:
		id_list_extended.append(id_)
	elif id_to_categories[id_] in req_four:
		id_list_extended.extend([id_] * 3)
		
id_list += id_list_extended

# Store all results
all_results = []

# Flag to keep running the while loop, i.e. keep extracting results
keep_running = True

# Track how many requests are being made
num_requests = 0

while keep_running:
	# Increment the number of requests that were made
	num_requests += 1
	
	# Generate the category set
	category_set = generate_category_set(list(set(id_list)))
	
	# Create the request
	URL = create_request(lat, lng, category_set)
	
	# Extract the results and add them to all results first
	nearby_results = requests.get(URL)
	nearby_results = nearby_results.json()
	
	all_results.extend(nearby_results['results'])

	# See which ids were found, knock them off the list if so
	found = []
	for result in nearby_results['results']:
		found.append(result['poi']['categorySet'][0]['id'])
		
	# Convert to counter objects so we can remove the exact number
	found_counter = Counter(found)
	id_list_counter = Counter(id_list)
	
	diff = id_list_counter - found_counter
	id_list = list(diff.elements())
	
	# Two conditions to terminate the loop: 1. id_list is empty, 2. No more results to retrieve
	
	if (not id_list) or (nearby_results['summary']['numResults'] < 100) or (num_requests>5):
		keep_running = False
				
	
return all_results, num_requests
										</code>
									</pre>

									<h2>Challege Three: A Lot of Data Cleaning!</h2>

									<p>
										Building a seamless data pipeline requires a lot of data cleaning and transformation, especially when collecting data from different sources. For example, one of my tasks joining a table detailing the population percentage over 65 from
										 <a href="http://census.gov">census.gov</a> into the main database. 
										 <img src="../images/joining problems.png" style="display:block; margin: auto; padding-bottom: 2%;" alt="">
										 For example, one of the many problems identified was that there were different naming conventions for individual cities. This caused an error in the left_join producing N/A values. 
										 Although these problems can be remedied by a single line of code (see below), it is often spotting these issues that can be a real problem, which requires intuition and granular investigation.
<pre>
	<code>
df['City'] = data['City'].str.replace("St.", "Saint")
	</code>
</pre>
									</p>

									<h2>Challenge Four: API's requests can be very slow. This is because </h2>

									<p>
										Requests can be very slow! Total extraction time for all columns took about 15 hours total. Luckily I could run these overnight and I only had to extract once. The long duration is due to the synchronous nature of the requests library which must 
										create a new session with the client server, make a request, wait for its response and close the session before starting it all over again. 
										 
									</p>
									<p>
										A faster way would be to implement requests asynchronously through <b> asyncio and aiohttp.</b> These enable the user to make a request and "await" responses as they arrive, 
										meanwhile, the program can send more requests allowing a  much more efficient data collection as no time is spent waiting for a response before the next request is sent. 
									</p>
									<img src="../images/syncvsasync.png" style="display:block; margin: auto; padding-bottom: 2%;" alt="">
									<p>
										I had implemented these libraries before, but it did not work in this instance possibly due to request limits. Maybe solving this issue for mainstream APIs will be the subject of a future post!
									</p>

								</div>



								<!--
								<footer>
									<ul class="stats">
										<li><a href="#">General</a></li>
										<li><a href="#" class="icon solid fa-heart">28</a></li>
										<li><a href="#" class="icon solid fa-comment">128</a></li>
									</ul>
								</footer>
								-->
							</article>

					</div>

				<!-- Footer -->
					<section id="footer">
						<ul class="icons">
							
							<li><a href="https://github.com/edmundhhn"><i class="fa fa-github fa-2x"></i></a></li>
							
						</ul>
						<!-- <p class="copyright">&copy; Untitled. Design: <a href="http://html5up.net">HTML5 UP</a>. Images: <a href="http://unsplash.com">Unsplash</a>.</p> -->
					</section>

			</div>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>